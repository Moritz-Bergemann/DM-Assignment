{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import data_prep\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Get the preprocessed dataset\n",
    "df = data_prep.get_cleaned_dataset()\n",
    "\n",
    "print(df.dtypes)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#####################\n",
    "### FUNCTION DEFS ###\n",
    "#####################\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prep_dataset(df, test_size=0.2):\n",
    "    # Separate labels & classes\n",
    "    X = df.drop('Class', axis=1).values     # Labels\n",
    "    y = df['Class'].values                  # Classes\n",
    "\n",
    "    X = OneHotEncoder().fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "from sklearn.metrics import confusion_matrix # TODO confusion matrix\n",
    "\n",
    "def basic_predict(X_train, X_test, y_train, y_test, classifier):\n",
    "    \"\"\"\n",
    "    Runs basic prediction with \n",
    "    :returns double indicating performance\n",
    "    NOTE: Random train/test split means performance is not consistent\n",
    "    \"\"\"\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_test_predict = classifier.predict(X_test)\n",
    "\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "    result = np.array([y_test_predict[ii] == y_test[ii] for ii in range(len(y_test))])\n",
    "\n",
    "    performance = np.count_nonzero(result)/len(result)\n",
    "\n",
    "    return performance"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Dataset\n",
    "df = data_prep.get_cleaned_dataset()\n",
    "\n",
    "X_train, X_test, y_train, y_test = prep_dataset(df, test_size=0.9966)\n",
    "\n",
    "# K-Nearest Neighbours\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_perf = basic_predict(X_train, X_test, y_train, y_test, knn)\n",
    "\n",
    "# Decision Tree \n",
    "# TODO use something other than decision tree\n",
    "dt = DecisionTreeClassifier()\n",
    "dt_perf = basic_predict(X_train, X_test, y_train, y_test, dt)\n",
    "\n",
    "# Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "gnb_perf = basic_predict(X_train, X_test, y_train, y_test, dt)\n",
    "\n",
    "print(f\"knn performance: {knn_perf}\")\n",
    "print(f\"dt performance: {dt_perf}\")\n",
    "print(f\"gnb performance: {gnb_perf}\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "###########\n",
    "### OLD ###\n",
    "###########\n",
    "\n",
    "# Try some knn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "train_test_ratio = 0.8\n",
    "\n",
    "performance_arr = []\n",
    "\n",
    "for n in range(1, 101):\n",
    "    knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "    last_train_row = int(len(df) * train_test_ratio)\n",
    "\n",
    "    X = df.drop('Class', axis=1).values     # Labels\n",
    "    y = df['Class'].values                  # Classes\n",
    "\n",
    "    X = OneHotEncoder().fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    y_predict = knn.predict(X_test)\n",
    "\n",
    "    result = np.array([y_predict[ii] == y_test[ii] for ii in range(len(y_test))])\n",
    "\n",
    "    performance = np.count_nonzero(result)/len(result)\n",
    "\n",
    "    performance_arr.append(performance)\n",
    "\n",
    "best_n = performance_arr.index(max(performance_arr))\n",
    "print(best_n)\n",
    "\n",
    "print(performance_arr[best_n])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "#########################################\n",
    "## Pipeline 1: K-Fold Cross-Validation ##\n",
    "#########################################\n",
    "## Missing: hparams, attribute selection, meta-classifiers ##\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import data_prep\n",
    "\n",
    "# Get dataset (mix of categorical & numeric)\n",
    "df_train, df_test = data_prep.get_prepped_dataset(bins=10, verbose=False)\n",
    "\n",
    "full = df_train.to_numpy()\n",
    "\n",
    "# Define values & labels\n",
    "X = df_train.drop('Class', axis=1).to_numpy()\n",
    "y = df_train['Class'].to_numpy()\n",
    "\n",
    "# Perform one-hot encoding on categorical values\n",
    "X = OneHotEncoder().fit_transform(X)\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "result = cross_val_score(knn, X, y, cv=5)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.70555556 0.72222222 0.70555556 0.68333333 0.72777778]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#############################################################\n",
    "## Pipeline 2: K-Fold Cross-Validation & Feature Selection ##\n",
    "#############################################################\n",
    "## Missing: hparams, attribute selection, meta-classifiers ##\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20) TODO do this later!!!\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import data_prep\n",
    "\n",
    "# Get dataset (mix of categorical & numeric)\n",
    "df_train, df_test = data_prep.get_prepped_dataset(bins=10, verbose=False)\n",
    "\n",
    "full = df_train.to_numpy()\n",
    "\n",
    "# Define values & labels\n",
    "X = df_train.drop('Class', axis=1).to_numpy()\n",
    "y = df_train['Class'].to_numpy()\n",
    "\n",
    "# Perform one-hot encoding on categorical values\n",
    "X = OneHotEncoder().fit_transform(X)\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Try removing all 2 pairs of features to see what gets the best performance\n",
    "print(\"Starting feature selection...\")\n",
    "sfs = SequentialFeatureSelector(knn, direction='backward', n_features_to_select=len(df_train.columns) - 1, cv=5)\n",
    "sfs.fit(X, y)\n",
    "\n",
    "print(\"Done!\")\n",
    "with open('pipeline_2', 'wb') as f:\n",
    "    pickle.dump(sfs, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "import pickle\n",
    "\n",
    "# messing about with the created object\n",
    "\n",
    "with open('./pipeline_2', 'rb') as f:\n",
    "    sfs:SequentialFeatureSelector = pickle.load(f)\n",
    "\n",
    "print(sfs.get_support())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[False  True  True False False False False False False False  True False\n",
      "  True False False False False False False False False False  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True  True False False  True False False\n",
      "  True  True  True False False False False False False False False False\n",
      " False False False False False False  True False False False False False\n",
      " False False False False  True False False False False False False False\n",
      "  True False False False False False False False  True False  True  True\n",
      "  True False False  True False False False False False False False False\n",
      " False False False False  True  True False False False False]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "####################################################################\n",
    "## Pipeline 3: K-Fold Cross-Validation, Feature Selection & SMOTE ##\n",
    "####################################################################\n",
    "## Missing: hparams, attribute selection, meta-classifiers ##\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "import data_prep\n",
    "\n",
    "\n",
    "# Get dataset (mix of categorical & numeric)\n",
    "df_train, df_test = data_prep.get_prepped_dataset(bins=10, verbose=False)\n",
    "\n",
    "full = df_train.to_numpy()\n",
    "\n",
    "# Define values & labels\n",
    "X = df_train.drop('Class', axis=1).to_numpy()\n",
    "y = df_train['Class'].to_numpy()\n",
    "\n",
    "# Perform one-hot encoding on categorical values\n",
    "X = OneHotEncoder().fit_transform(X)\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Try removing all 2 pairs of features to see what gets the best performance\n",
    "print(\"Starting feature selection...\")\n",
    "sfs = SequentialFeatureSelector(knn, direction='backward', n_features_to_select=len(df_train.columns) - 1, cv=5)\n",
    "sfs.fit(X, y)\n",
    "\n",
    "print(\"Done!\")\n",
    "with open('pipeline_2', 'wb') as f:\n",
    "    pickle.dump(sfs, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "arr = np.array(['A', 'B', 'C', 'A', 'C'])\n",
    "\n",
    "arr = arr.reshape(-1, 1)\n",
    "\n",
    "arr_oh = OneHotEncoder().fit_transform(arr)\n",
    "\n",
    "print(arr_oh)\n",
    "print(type(arr_oh))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 2)\t1.0\n",
      "  (3, 0)\t1.0\n",
      "  (4, 2)\t1.0\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}