{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import data_prep\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Get the preprocessed dataset\n",
    "df = data_prep.get_cleaned_dataset()\n",
    "\n",
    "print(df.dtypes)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#####################\n",
    "### FUNCTION DEFS ###\n",
    "#####################\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prep_dataset(df, test_size=0.2):\n",
    "    # Separate labels & classes\n",
    "    X = df.drop('Class', axis=1).values     # Labels\n",
    "    y = df['Class'].values                  # Classes\n",
    "\n",
    "    X = OneHotEncoder().fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "from sklearn.metrics import confusion_matrix # TODO confusion matrix\n",
    "\n",
    "def basic_predict(X_train, X_test, y_train, y_test, classifier):\n",
    "    \"\"\"\n",
    "    Runs basic prediction with \n",
    "    :returns double indicating performance\n",
    "    NOTE: Random train/test split means performance is not consistent\n",
    "    \"\"\"\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_test_predict = classifier.predict(X_test)\n",
    "\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "    result = np.array([y_test_predict[ii] == y_test[ii] for ii in range(len(y_test))])\n",
    "\n",
    "    performance = np.count_nonzero(result)/len(result)\n",
    "\n",
    "    return performance"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Dataset\n",
    "df = data_prep.get_cleaned_dataset()\n",
    "\n",
    "X_train, X_test, y_train, y_test = prep_dataset(df, test_size=0.9966)\n",
    "\n",
    "# K-Nearest Neighbours\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_perf = basic_predict(X_train, X_test, y_train, y_test, knn)\n",
    "\n",
    "# Decision Tree \n",
    "# TODO use something other than decision tree\n",
    "dt = DecisionTreeClassifier()\n",
    "dt_perf = basic_predict(X_train, X_test, y_train, y_test, dt)\n",
    "\n",
    "# Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "gnb_perf = basic_predict(X_train, X_test, y_train, y_test, dt)\n",
    "\n",
    "print(f\"knn performance: {knn_perf}\")\n",
    "print(f\"dt performance: {dt_perf}\")\n",
    "print(f\"gnb performance: {gnb_perf}\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "###########\n",
    "### OLD ###\n",
    "###########\n",
    "\n",
    "# Try some knn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "train_test_ratio = 0.8\n",
    "\n",
    "performance_arr = []\n",
    "\n",
    "for n in range(1, 101):\n",
    "    knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "    last_train_row = int(len(df) * train_test_ratio)\n",
    "\n",
    "    X = df.drop('Class', axis=1).values     # Labels\n",
    "    y = df['Class'].values                  # Classes\n",
    "\n",
    "    X = OneHotEncoder().fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    y_predict = knn.predict(X_test)\n",
    "\n",
    "    result = np.array([y_predict[ii] == y_test[ii] for ii in range(len(y_test))])\n",
    "\n",
    "    performance = np.count_nonzero(result)/len(result)\n",
    "\n",
    "    performance_arr.append(performance)\n",
    "\n",
    "best_n = performance_arr.index(max(performance_arr))\n",
    "print(best_n)\n",
    "\n",
    "print(performance_arr[best_n])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "#########################################\n",
    "## Pipeline 1: K-Fold Cross-Validation ##\n",
    "#########################################\n",
    "## Missing: hparams, attribute selection, meta-classifiers ##\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import data_prep\n",
    "\n",
    "# Get dataset (mix of categorical & numeric)\n",
    "df_train, df_test = data_prep.get_prepped_dataset(bins=10, verbose=False)\n",
    "\n",
    "full = df_train.to_numpy()\n",
    "\n",
    "# Define values & labels\n",
    "X = df_train.drop('Class', axis=1).to_numpy()\n",
    "y = df_train['Class'].to_numpy()\n",
    "\n",
    "# Perform one-hot encoding on categorical values\n",
    "X = OneHotEncoder().fit_transform(X)\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "result = cross_val_score(knn, X, y, cv=5)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.70555556 0.72222222 0.70555556 0.68333333 0.72777778]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#############################################################\n",
    "## Pipeline 2: K-Fold Cross-Validation & Feature Selection ##\n",
    "#############################################################\n",
    "## Missing: hparams, attribute selection, meta-classifiers ##\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20) TODO do this later!!!\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import data_prep\n",
    "\n",
    "# Get dataset (mix of categorical & numeric)\n",
    "df_train, df_test = data_prep.get_prepped_dataset(bins=10, verbose=False)\n",
    "\n",
    "full = df_train.to_numpy()\n",
    "\n",
    "# Define values & labels\n",
    "X = df_train.drop('Class', axis=1).to_numpy()\n",
    "y = df_train['Class'].to_numpy()\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Build pipeline\n",
    "pl = Pipeline([\n",
    "    ('onehot', OneHotEncoder()),\n",
    "    ('sfs', SequentialFeatureSelector(knn, direction='backward', n_features_to_select=None, cv=5)) # TODO change these params\n",
    "])\n",
    "\n",
    "print(\"Starting pipeline fit...\")\n",
    "pl.fit(X, y)\n",
    "\n",
    "# Try removing all 2 pairs of features to see what gets the best performance\n",
    "print(\"Done!\")\n",
    "with open('pipeline_2', 'wb') as f:\n",
    "    pickle.dump(pl, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "###############\n",
    "## NEW THING ##\n",
    "###############\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "\n",
    "# messing about with the created object\n",
    "df_train, df_test = data_prep.get_prepped_dataset(bins=10, verbose=False)\n",
    "\n",
    "with open('./pipeline_2', 'rb') as f:\n",
    "    result:Pipeline = pickle.load(f)\n",
    "\n",
    "sfs:SequentialFeatureSelector = result.named_steps['sfs']\n",
    "print(sfs.get_support().shape)\n",
    "\n",
    "# test = df_train.to_numpy()\n",
    "# X = df_train.drop('Class', axis=1).to_numpy()\n",
    "# y = df_train['Class'].to_numpy()\n",
    "\n",
    "# score = accuracy_score(result.predict(X), y)\n",
    "# print(score)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(118,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "####################################################################\n",
    "## Pipeline 3: K-Fold Cross-Validation & SMOTE ##\n",
    "####################################################################\n",
    "## Missing: hparams, attribute selection, meta-classifiers ##\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, StratifiedKFold, KFold, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE, SMOTEN\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "\n",
    "import data_prep\n",
    "\n",
    "\n",
    "# Get dataset (mix of categorical & numeric)\n",
    "df_train, df_test = data_prep.get_prepped_dataset(bins=10, verbose=False)\n",
    "\n",
    "full = df_train.to_numpy()\n",
    "\n",
    "# Define values & labels\n",
    "X = df_train.drop('Class', axis=1).to_numpy()\n",
    "y = df_train['Class'].to_numpy()\n",
    "\n",
    "# Label-encode the labels  \n",
    "y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "# Build imblearn pipeline with nominal SMOTE\n",
    "# ipl = make_pipeline(\n",
    "#     SMOTEN(random_state=123, sampling_strategy='not minority'),\n",
    "#     OneHotEncoder()\n",
    "# )\n",
    "\n",
    "# X_r, y_r = ipl.fit_resample(X, y)\n",
    "\n",
    "# for col in df_train.drop('Class', axis=1).columns:\n",
    "#     print(list(df_train[col].cat.categories))\n",
    "\n",
    "# Get list of categories per column\n",
    "categories = [list(df_train[col].cat.categories) for col in df_train.drop('Class', axis=1).columns]\n",
    "\n",
    "pl = imbpipeline(steps= [\n",
    "        ('smoten', SMOTEN(random_state=123, sampling_strategy='not majority')),\n",
    "        ('onehot', OneHotEncoder(categories=categories)),\n",
    "        ('knn', KNeighborsClassifier())\n",
    "    ]\n",
    ")\n",
    "\n",
    "pl.fit(X, y)\n",
    "\n",
    "# K-Fold Cross validation generator\n",
    "# NOTE: Stratified K-Fold used to ensure training classes are balanced (gives better representation of SMOTE-enabled prediction)\n",
    "# kfold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "param_grid = {\n",
    "    'knn__n_neighbors': [ii for ii in range(3,5)]\n",
    "}\n",
    "grid_search = GridSearchCV(pl, param_grid)\n",
    "\n",
    "print(\"Starting fit..\")\n",
    "grid_search.fit(X, y)\n",
    "print(\"Done!\")\n",
    "\n",
    "with open('pipeline_3_graph_search_knn', 'wb') as f:\n",
    "    pickle.dump(grid_search, f)\n",
    "\n",
    "# BELOW WORKY\n",
    "# scores = cross_validate(pl, X, y, cv=kfold, scoring=('accuracy', 'balanced_accuracy', 'f1', 'precision', 'recall'))\n",
    "# print(\"SCORES\")\n",
    "# print(scores)\n",
    "\n",
    "# preds = cross_val_predict(pl, X, y, cv=kfold)\n",
    "\n",
    "# conf_mat = confusion_matrix(y_pred=preds, y_true=y, normalize='all')\n",
    "# print(\"CONFUSION MATRIX\")\n",
    "# print(conf_mat)\n",
    "\n",
    "# sns.heatmap(conf_mat, annot=True)\n",
    "\n",
    "# Get label proportions\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "counts = np.array([val/len(y) for val in counts])\n",
    "label_proportions = dict(zip(unique, counts))\n",
    "print(label_proportions)\n",
    "\n",
    "\n",
    "# THIS WORKY ######################################################\n",
    "# print(f\"Pre-SMOTE shape: {X.shape}\")\n",
    "\n",
    "# smoten = SMOTEN(random_state=123, sampling_strategy='not majority')\n",
    "# X_r, y_r = smoten.fit_resample(X, y)\n",
    "\n",
    "# print(f\"Post-SMOTE shape: {X_r.shape}\")\n",
    "\n",
    "# oh_enc = OneHotEncoder()\n",
    "# X_r = oh_enc.fit_transform(X_r)\n",
    "\n",
    "# print(f\"Post-Onehot shape: {X_r.shape}\")\n",
    "\n",
    "# l_enc = LabelEncoder()\n",
    "# y_r = l_enc.fit_transform(y_r)\n",
    "# /THIS WORKY #####################################################\n",
    "\n",
    "# print(X_r)\n",
    "# np.savetxt('./x.txt', X_r, fmt='%s')\n",
    "\n",
    "# print(\"Done!\")\n",
    "# with open('pipeline_2', 'wb') as f:\n",
    "#     pickle.dump(sfs, f)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting fit..\n",
      "Done!\n",
      "{0: 0.7222222222222222, 1: 0.2777777777777778}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "# Messing about with grid search output\n",
    "\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "import pickle\n",
    "\n",
    "with open('./pipeline_3_graph_search_knn', 'rb') as f:\n",
    "    r:imbpipeline = pickle.load(f)\n",
    "\n",
    "r.cv_results_"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.14563704, 0.14527011, 0.14538217, 0.14670243, 0.14642553,\n",
       "        0.14640503, 0.14465084, 0.14359536, 0.14569526, 0.14511809,\n",
       "        0.14548187, 0.14464917, 0.14447303, 0.14507599, 0.14581785,\n",
       "        0.14637213, 0.14540472, 0.14490886, 0.14604597]),\n",
       " 'std_fit_time': array([0.00274631, 0.0005022 , 0.00167734, 0.00353577, 0.00351841,\n",
       "        0.00200359, 0.00232369, 0.00210118, 0.00232087, 0.00210185,\n",
       "        0.00280988, 0.00168527, 0.00252989, 0.00131327, 0.00189256,\n",
       "        0.0015224 , 0.0017069 , 0.00189405, 0.00200904]),\n",
       " 'mean_score_time': array([0.01212091, 0.01228065, 0.01290884, 0.01261086, 0.01274905,\n",
       "        0.01294932, 0.01286087, 0.01273909, 0.01277547, 0.01304007,\n",
       "        0.01288052, 0.01284432, 0.01263766, 0.0129756 , 0.01338882,\n",
       "        0.01301546, 0.01298103, 0.01299376, 0.01298695]),\n",
       " 'std_score_time': array([3.25947250e-04, 3.77436428e-04, 3.03362271e-04, 3.75344858e-04,\n",
       "        4.86528536e-04, 3.83040787e-04, 3.86740087e-04, 3.75382996e-04,\n",
       "        3.11501676e-04, 1.99376387e-04, 1.70595609e-04, 2.86047811e-04,\n",
       "        4.87605107e-05, 2.62513845e-04, 1.00538003e-03, 3.22740121e-04,\n",
       "        4.37108370e-04, 2.09438334e-04, 1.51935120e-04]),\n",
       " 'param_knn__n_neighbors': masked_array(data=[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "                    18, 19, 20],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'knn__n_neighbors': 2},\n",
       "  {'knn__n_neighbors': 3},\n",
       "  {'knn__n_neighbors': 4},\n",
       "  {'knn__n_neighbors': 5},\n",
       "  {'knn__n_neighbors': 6},\n",
       "  {'knn__n_neighbors': 7},\n",
       "  {'knn__n_neighbors': 8},\n",
       "  {'knn__n_neighbors': 9},\n",
       "  {'knn__n_neighbors': 10},\n",
       "  {'knn__n_neighbors': 11},\n",
       "  {'knn__n_neighbors': 12},\n",
       "  {'knn__n_neighbors': 13},\n",
       "  {'knn__n_neighbors': 14},\n",
       "  {'knn__n_neighbors': 15},\n",
       "  {'knn__n_neighbors': 16},\n",
       "  {'knn__n_neighbors': 17},\n",
       "  {'knn__n_neighbors': 18},\n",
       "  {'knn__n_neighbors': 19},\n",
       "  {'knn__n_neighbors': 20}],\n",
       " 'split0_test_score': array([0.71666667, 0.67222222, 0.64444444, 0.61666667, 0.67222222,\n",
       "        0.65      , 0.65555556, 0.65      , 0.66666667, 0.63333333,\n",
       "        0.64444444, 0.61111111, 0.63333333, 0.61666667, 0.62777778,\n",
       "        0.58333333, 0.61111111, 0.59444444, 0.58888889]),\n",
       " 'split1_test_score': array([0.73333333, 0.65555556, 0.66666667, 0.65555556, 0.71111111,\n",
       "        0.68333333, 0.69444444, 0.68333333, 0.69444444, 0.68333333,\n",
       "        0.67777778, 0.67222222, 0.68333333, 0.67222222, 0.68333333,\n",
       "        0.66111111, 0.66666667, 0.64444444, 0.64444444]),\n",
       " 'split2_test_score': array([0.70555556, 0.66666667, 0.63888889, 0.61111111, 0.63333333,\n",
       "        0.58333333, 0.61666667, 0.6       , 0.60555556, 0.55555556,\n",
       "        0.57222222, 0.56666667, 0.57777778, 0.58888889, 0.61111111,\n",
       "        0.60555556, 0.58888889, 0.57222222, 0.58333333]),\n",
       " 'split3_test_score': array([0.63888889, 0.63888889, 0.61666667, 0.59444444, 0.65555556,\n",
       "        0.59444444, 0.65555556, 0.6       , 0.62222222, 0.61666667,\n",
       "        0.63888889, 0.63333333, 0.64444444, 0.61111111, 0.64444444,\n",
       "        0.61666667, 0.64444444, 0.62222222, 0.61666667]),\n",
       " 'split4_test_score': array([0.75555556, 0.71111111, 0.68888889, 0.66666667, 0.69444444,\n",
       "        0.62777778, 0.65555556, 0.61111111, 0.65      , 0.61666667,\n",
       "        0.63333333, 0.58333333, 0.60555556, 0.57777778, 0.59444444,\n",
       "        0.57777778, 0.58888889, 0.57222222, 0.6       ]),\n",
       " 'mean_test_score': array([0.71      , 0.66888889, 0.65111111, 0.62888889, 0.67333333,\n",
       "        0.62777778, 0.65555556, 0.62888889, 0.64777778, 0.62111111,\n",
       "        0.63333333, 0.61333333, 0.62888889, 0.61333333, 0.63222222,\n",
       "        0.60888889, 0.62      , 0.60111111, 0.60666667]),\n",
       " 'std_test_score': array([0.03934651, 0.02398559, 0.02469568, 0.02753225, 0.02753225,\n",
       "        0.03651484, 0.02459549, 0.03284832, 0.0315446 , 0.04088527,\n",
       "        0.03424674, 0.03728436, 0.03572874, 0.03269764, 0.03051007,\n",
       "        0.02973131, 0.03095197, 0.02841492, 0.02205493]),\n",
       " 'rank_test_score': array([ 1,  3,  5, 10,  2, 12,  4,  9,  6, 13,  7, 15, 10, 16,  8, 17, 14,\n",
       "        19, 18], dtype=int32)}"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "###############################\n",
    "## ATTEMPT AT FULL FUNCTIONS ##\n",
    "###############################\n",
    "\n",
    "# TODO clean these up\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, StratifiedKFold, KFold, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE, SMOTEN\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "\n",
    "import data_prep\n",
    "\n",
    "RANDOM_STATE = 123\n",
    "# TODO don't be a perfectionist!\n",
    "\n",
    "# Non Sklearn-adjustable hyperparams go in here!!!\n",
    "def trial(model:str, drop_attributes:list, sampling:str, bins:int, cat_binning_threshold:int): # TODO pick h-params to adjust\n",
    "    df_train, df_test = data_prep.get_prepped_dataset(bins=bins, cat_binning_threshold=cat_binning_threshold)\n",
    "\n",
    "    # Input validation\n",
    "    assert sampling in ['none', 'over', 'under', 'both']\n",
    "    assert model in ['knn', 'dt', 'nbayes', 'svm']\n",
    "\n",
    "    ## Prepare Dataset\n",
    "\n",
    "    # Split into labels/not labels\n",
    "    df_X = df_train.drop('Class', axis=1)\n",
    "    df_y = df_train['Class']\n",
    "\n",
    "    X = df_X.to_numpy()\n",
    "    y = df_y.to_numpy()\n",
    "\n",
    "    # Encode labels\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "    ## Make data pipeline\n",
    "    pipe_parts = []\n",
    "\n",
    "    # Sampling strategy\n",
    "    if bins != None: # If everything binned\n",
    "        if sampling == 'over':\n",
    "            pipe_parts.append(('smoten', SMOTEN(random_state=RANDOM_STATE, sampling_strategy='not majority')))\n",
    "        elif sampling == 'under':\n",
    "            raise NotImplementedError() # TODO\n",
    "        elif sampling == 'both':\n",
    "            raise NotImplementedError() # TODO\n",
    "    else: # Add Smote-Mixed if things not binned\n",
    "        raise NotImplementedError() # TODO\n",
    "    \n",
    "    # One-hot encoding\n",
    "    if bins != None: # Use onehot encoder if everything binned\n",
    "        pipe_parts.append('onehot', OneHotEncoder())\n",
    "    else: # Use mixed encoder if not everything binned\n",
    "        raise NotImplementedError() # TODO\n",
    "    \n",
    "    # Model itself\n",
    "    if model =='knn':\n",
    "        pipe_parts.append('model_knn', KNeighborsClassifier())\n",
    "    if model =='dt':\n",
    "        pipe_parts.append('model_dt', DecisionTreeClassifier())\n",
    "    if model =='nbayes':\n",
    "        raise NotImplementedError\n",
    "        pipe_parts.append('model_nbayes', ) # TODO\n",
    "    if model =='svm':\n",
    "        raise NotImplementedError\n",
    "        pipe_parts.append('model_svm', ) # TODO\n",
    "\n",
    "    # Make final pipeline\n",
    "    pipe = ImbPipeline(pipe_parts)\n",
    "\n",
    "    ## Add grid search feature selection\n",
    "    if model =='knn':\n",
    "        param_grid = {\n",
    "            # All odd values for k-neighbors between 3 & 22\n",
    "            'knn__k_neighbors': [ii for ii in range(3, 22, 2)]\n",
    "        }\n",
    "    if model =='dt':\n",
    "        param_grid = {\n",
    "            'dt__criterion': ['gini', 'entropy'],\n",
    "            'dt__min_samples_split': [ii for ii in range(2, 41)],\n",
    "            'dt__min_samples_leaf': [ii for ii in range(1, 21)]\n",
    "        }\n",
    "    if model =='nbayes':\n",
    "        param_grid = {\n",
    "        }\n",
    "    if model =='svm':\n",
    "        raise NotImplementedError\n",
    "        param_grid = {\n",
    "\n",
    "        }\n",
    "    grid_search = GridSearchCV(pipe, param_grid, verbose=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "arr = np.array(['A', 'B', 'C', 'A', 'C'])\n",
    "\n",
    "arr = arr.reshape(-1, 1)\n",
    "\n",
    "arr_oh = OneHotEncoder().fit_transform(arr)\n",
    "\n",
    "print(arr_oh)\n",
    "print(type(arr_oh))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 2)\t1.0\n",
      "  (3, 0)\t1.0\n",
      "  (4, 2)\t1.0\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "l = ['A', 'B', 'C', 'D']\n",
    "l2 = ['B', 'C']\n",
    "\n",
    "set(l2) <= set(l)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}